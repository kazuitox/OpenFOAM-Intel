#!/bin/sh
#SBATCH -J mesh
#SBATCH -o mesh.o%J
#SBATCH -e mesh.e%J
#SBATCH --ntasks=144
#SBATCH --constraint cluster-size-4,hpc3

cd ${SLURM_SUBMIT_DIR}
scontrol show hostname $SLURM_JOB_NODELIST | sed 's/$/ slots=36/' > ./hostfile
./generate-ompi-rankfile.sh

NPROCS=36
NX=60
NY=24
NZ=24

source /usr/mpi/gcc/openmpi-4.0.3rc4/bin/mpivars.sh
source /nfs/cluster/app/v2006/OpenFOAM/OpenFOAM-v2006/etc/bashrc

export HOSTFILE="./hostfile"
export RANKFILE="./rankfile"
export MPI=$(which mpirun)
export MPI_BASE_OPTIONS="-np $NPROCS -hostfile $HOSTFILE -rankfile ${RANKFILE}"
#export MPI_RoCEv2_OPTIONS="-mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3"
export MPI_RoCEv2_OPTIONS="-mca pml ucx -x UCX_NET_DEVICES=mlx5_2:1 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -x HCOLL_ENABLE_MCAST_ALL=0 -x coll_hcoll_enable=0"
#export MPI_Pinning_OPTIONS="--cpu-set 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35"
#export MPI_Pinning_OPTIONS="--use-hwthread-cpus"
export MPI_OPTIONS="${MPI_BASE_OPTIONS} ${MPI_RoCEv2_OPTIONS} ${MPI_Pinning_OPTIONS}"


echo "Setting up with $NPROCS procs.  Blockmesh size = $NX x $NY x $NZ"

mkdir -p constant/polyMesh
mkdir -p constant/triSurface

sed "s/NX/$NX/g;s/NY/$NY/g;s/NZ/$NZ/g" system/blockMeshDict-mesh.in > constant/polyMesh/blockMeshDict
cp system/decomposeParDict-mesh.in system/decomposeParDict

# Source tutorial run functions
. $WM_PROJECT_DIR/bin/tools/RunFunctions

# copy motorbike surface from resources directory
cp $FOAM_TUTORIALS/resources/geometry/motorBike.obj.gz constant/triSurface/
runApplication surfaceFeatureExtract

runApplication blockMesh

runApplication decomposePar
#runParallel snappyHexMesh -overwrite
${MPI} ${MPI_OPTIONS} snappyHexMesh -overwrite -parallel >& log.snappyHexMesh

runApplication reconstructParMesh -constant
rm -rf processor*

runApplication renumberMesh -constant -overwrite

