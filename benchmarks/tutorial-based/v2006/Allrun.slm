#!/bin/sh
#SBATCH -J motorbike
#SBATCH -o motorbike.o%J
#SBATCH -e motorbike.e%J
#SBATCH --ntasks=72
#SBATCH --constraint cluster-size-2,hpc3

MESH_NPROCS=36
NX=60
NY=24
NZ=24

source /usr/mpi/gcc/openmpi-4.0.3rc4/bin/mpivars.sh
source /nfs/cluster/app/v2.2.2/OpenFOAM/OpenFOAM-2.2.2/etc/bashrc
export HOSTFILE="./hostfile"
export MPI=$(which mpirun)
export MPI_BASE_OPTIONS="-np $MESH_NPROCS -hostfile $HOSTFILE"
#export MPI_RoCEv2_OPTIONS="-mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3"
export MPI_RoCEv2_OPTIONS="-mca pml ucx -x UCX_NET_DEVICES=mlx5_2:1 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -x HCOLL_ENABLE_MCAST_ALL=0 -x coll_hcoll_enable=0"
export MPI_Pinning_OPTIONS="--cpu-set 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35"
#export MPI_Pinning_OPTIONS="--use-hwthread-cpus"
export MPI_OPTIONS="${MPI_BASE_OPTIONS} ${MPI_RoCEv2_OPTIONS} ${MPI_Pinning_OPTIONS}"

cd ${SLURM_SUBMIT_DIR}
scontrol show hostname $SLURM_JOB_NODELIST | sed 's/$/ slots=36/' > ./hostfile

# Source tutorial clean functions
. $WM_PROJECT_DIR/bin/tools/CleanFunctions

# remove surface and features
\rm -rf constant/extendedFeatureEdgeMesh > /dev/null 2>&1
\rm -f constant/triSurface/motorBike.eMesh > /dev/null 2>&1

rm -rf 0 > /dev/null 2>&1

cleanCase

##
## Mesh
## 

echo "Setting up with $MESH_NPROCS procs.  Blockmesh size = $NX x $NY x $NZ"

mkdir -p constant/polyMesh
mkdir -p constant/triSurface

sed "s/NX/$NX/g;s/NY/$NY/g;s/NZ/$NZ/g" system/blockMeshDict-mesh.in > constant/polyMesh/blockMeshDict
cp system/decomposeParDict-mesh.in system/decomposeParDict

# copy motorbike surface from resources directory
cp $FOAM_TUTORIALS/resources/geometry/motorBike.obj.gz constant/triSurface/
runApplication surfaceFeatureExtract

runApplication blockMesh

runApplication decomposePar
#runParallel snappyHexMesh -overwrite
${MPI} ${MPI_OPTIONS} snappyHexMesh -overwrite -parallel >& log.snappyHexMesh

runApplication reconstructParMesh -constant
rm -rf processor*

runApplication renumberMesh -constant -overwrite

##
## Setup
## 

NPROCS=${SLURM_NTASKS}
export MPI_BASE_OPTIONS="-np $NPROCS -hostfile $HOSTFILE --display-map"
#export MPI_RoCEv2_OPTIONS="-mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3"
export MPI_RoCEv2_OPTIONS="-mca pml ucx -x UCX_NET_DEVICES=mlx5_2:1 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -x HCOLL_ENABLE_MCAST_ALL=0 -x coll_hcoll_enable=0"
export MPI_Pinning_OPTIONS="--cpu-set 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35"
export MPI_X_OPTIONS=$(./generate-ompi-x-options.sh)
#export MPI_Pinning_OPTIONS="--use-hwthread-cpus"
export MPI_OPTIONS="${MPI_BASE_OPTIONS} ${MPI_X_OPTIONS} ${MPI_RoCEv2_OPTIONS} ${MPI_Pinning_OPTIONS}"


echo "Running with $NPROCS procs."

rm -f log.*
rm -rf processor*

sed "s/NPROCS/$NPROCS/g" system/decomposeParDict-solve.in > system/decomposeParDict

runApplication decomposePar

#- For non-parallel running
#cp -r 0.org 0 > /dev/null 2>&1

#- For parallel running
ls -d processor* | xargs -I {} rm -rf ./{}/0
ls -d processor* | xargs -I {} cp -r 0.org ./{}/0

#runParallel potentialFoam
${MPI} ${MPI_OPTIONS} potentialFoam -parallel >& log.potentialFoam

##
## Solve
##
${MPI} ${MPI_OPTIONS} simpleFoam -parallel  >& log.simpleFoam

# ----------------------------------------------------------------- end-of-file
