#!/bin/sh
#SBATCH -J solve
#SBATCH -o solve.o%J
#SBATCH -e solve.e%J
#SBATCH --ntasks=72
#SBATCH --constraint cluster-size-2,hpc3
cd ${SLURM_SUBMIT_DIR}
scontrol show hostname $SLURM_JOB_NODELIST | sed 's/$/ slots=36/' > ./hostfile

source /usr/mpi/gcc/openmpi-4.0.3rc4/bin/mpivars.sh
source /nfs/cluster/app/v2006/OpenFOAM/OpenFOAM-v2006/etc/bashrc

export NPROCS=${SLURM_NTASKS}
export HOSTFILE="./hostfile"
export RANKFILE="./rankfile"
export MPI=$(which mpirun)
#export MPI_BASE_OPTIONS="-np $NPROCS -hostfile $HOSTFILE -rankfile ${RANKFILE} --display-map"
export MPI_BASE_OPTIONS="-np $NPROCS -hostfile $HOSTFILE --display-map"
#export MPI_RoCEv2_OPTIONS="-mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3"
export MPI_RoCEv2_OPTIONS="-mca pml ucx -x UCX_NET_DEVICES=mlx5_2:1 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -x HCOLL_ENABLE_MCAST_ALL=0 -x coll_hcoll_enable=0"
export MPI_X_OPTIONS=$(./generate-ompi-x-options.sh)
#export MPI_Pinning_OPTIONS="--cpu-set 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35"
export MPI_Pinning_OPTIONS="--use-hwthread-cpus --map-by core"
export MPI_OPTIONS="${MPI_BASE_OPTIONS} ${MPI_X_OPTIONS} ${MPI_RoCEv2_OPTIONS} ${MPI_Pinning_OPTIONS}"



# Source tutorial run functions
. $WM_PROJECT_DIR/bin/tools/RunFunctions

# NOTE: To launch without the runParallel, do the following:
#
#           mpirun -np $NPROCS simpleFoam -parallel
#
#runParallel simpleFoam
${MPI} ${MPI_OPTIONS} simpleFoam -parallel  >& log.simpleFoam

# ----------------------------------------------------------------- end-of-file
