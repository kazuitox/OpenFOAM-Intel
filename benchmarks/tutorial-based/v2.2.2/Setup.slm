#!/bin/sh
#SBATCH -J setup
#SBATCH -o setup.o%J
#SBATCH -e setup.e%J
#SBATCH --ntasks=72
#SBATCH --constraint cluster-size-2,hpc3

cd ${SLURM_SUBMIT_DIR}
scontrol show hostname $SLURM_JOB_NODELIST | sed 's/$/ slots=36/' > ./hostfile

source /usr/mpi/gcc/openmpi-4.0.3rc4/bin/mpivars.sh
source /nfs/cluster/app/v2.2.2/OpenFOAM/OpenFOAM-2.2.2/etc/bashrc

NPROCS=${SLURM_NTASKS}
export HOSTFILE="./hostfile"
export RANKFILE="./rankfile"
export MPI=$(which mpirun)
export MPI_BASE_OPTIONS="-np $NPROCS -hostfile $HOSTFILE -rankfile ${RANKFILE} --display-map"
#export MPI_RoCEv2_OPTIONS="-mca btl self -x UCX_TLS=rc,self,sm -x HCOLL_ENABLE_MCAST_ALL=0 -mca coll_hcoll_enable 0 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3"
export MPI_RoCEv2_OPTIONS="-mca pml ucx -x UCX_NET_DEVICES=mlx5_2:1 -x UCX_IB_TRAFFIC_CLASS=105 -x UCX_IB_GID_INDEX=3 -x HCOLL_ENABLE_MCAST_ALL=0 -x coll_hcoll_enable=0"
#export MPI_Pinning_OPTIONS="--cpu-set 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35"
export MPI_X_OPTIONS=$(./generate-ompi-x-options.sh)
export MPI_Pinning_OPTIONS="--use-hwthread-cpus"
export MPI_OPTIONS="${MPI_BASE_OPTIONS} ${MPI_X_OPTIONS} ${MPI_RoCEv2_OPTIONS} ${MPI_Pinning_OPTIONS}"


echo "Running with $NPROCS procs."

rm -f log.*
rm -rf processor*

sed "s/NPROCS/$NPROCS/g" system/decomposeParDict-solve.in > system/decomposeParDict

# Source tutorial run functions
. $WM_PROJECT_DIR/bin/tools/RunFunctions

runApplication decomposePar

#- For non-parallel running
#cp -r 0.org 0 > /dev/null 2>&1

#- For parallel running
ls -d processor* | xargs -I {} rm -rf ./{}/0
ls -d processor* | xargs -I {} cp -r 0.org ./{}/0

#runParallel potentialFoam
${MPI} ${MPI_OPTIONS} potentialFoam -parallel >& log.potentialFoam

# ----------------------------------------------------------------- end-of-file
